# MLReproducibilityChallenge2021

In this repository we have tried to replicate the experimental results of the paper:

ðŸ“„ Unifying Vision-and-Language Tasks via Text Generation.  
Cho, J., Lei, J., Tan, H., & Bansal, M. (2021). Unifying Vision-and-Language Tasks via Text Generation. ICML.

Paper Link: https://arxiv.org/pdf/2102.02779.pdf

Github Repository of the paper: https://github.com/j-min/VL-T5

Due to the lack of resources we were only able to use pretrained models on the downstream tasks.

These downstream tasks are in below.
1- Visual Qestion Answering (VQA, GQA)
2- Natural Language Visual Reasoning (NLVR)
3- Visual Commonsense Reasoning (VCR)
4- Referring Expression Comprehension: (RefCOCOg)
5- Captioning (COCO)
6- En-De Machine Translation (Multi30k)

We have created have below notebooks in Google Colab.

COCO Captioning [https://colab.research.google.com/drive/14j36ZHWHUDqHYmLOoltX5Ytu-XrCpcZx?usp=sharing]
VCR  [https://colab.research.google.com/drive/1iIMhh_aZ3hGUUVTWS9eYZ5UPVYSPeYqs?usp=sharing]
VQA [https://colab.research.google.com/drive/13PGig2dE32Frmfpyc63fyCbx3V_nGjD7?usp=sharing]
GQA [https://colab.research.google.com/drive/12Dck4G9LKihkd6VRXhWMMf_GSqPNahIE?usp=sharing]
NLVR [https://colab.research.google.com/drive/1x5Y4wCLHOP3RC77kfb1BPWwuPmGqo6Uc?usp=sharing]
RefCOCOg [https://colab.research.google.com/drive/1m9YRwEaGPeciKDWSaFNcvDCNaN0acf2F?usp=sharing]
